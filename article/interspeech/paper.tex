\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2015}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{multirow}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}


\sloppy
\ninept


\title{What Makes a Speaker Recognizable in TV Broadcast?\\
Going Beyond Speaker Identification Error Rate}


% \title{Speakers identification in broadcast TV: facilities and barriers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\makeatletter
%\def\name#1{\gdef\@name{#1\\}}
%\makeatother
%\name{{\em Firstname1 Lastname1, Firstname2 Lastname2, Firstname3 Lastname3,}\\
%      {\em Firstname4 Lastname4, Firstname5 Lastname5, Firstname6 Lastname6,
%      Firstname7 Lastname7}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother \name{{\em Delphine Charlet$^{1}$, Johann Poignant$^{2}$, Herv\'{e} Bredin$^{2}$, Corinne Fredouille$^{3}$, Sylvain Meignier$^{4}$}}

\address{$^1$ Orange Labs -- Lannion, France\\
         $^2$ LIMSI -- CNRS -- Orsay, France.\\
         $^3$ CERI/LIA -- University of Avignon -- Avignon, France\\
         $^4$ LIUM -- Universit\'e du Mans -- Le Mans, France \\
         {\small \tt delphine.charlet@orange.com}
}


\begin{document}

\maketitle

\begin{abstract}
\vspace*{-0.2cm}
Speaker identification approaches for TV broadcast are usually evaluated and compared based on global error rates derived from the overall duration of missed detection, false alarm and confusion.
Based on the analysis of the output of the systems submitted to the final round of the French evaluation campaign REPERE, this paper highlights the fact that these average metrics lead to the incorrect intuition that current state-of-the-art algorithms partially recognize all speakers. Setting aside incorrect diarization and adverse acoustic conditions, we show that their performance is in fact essentially bi-modal: in a given show, either all speech turns of a speaker are correctly identified or none of them are. We then proceed with trying to understand and explain this behavior, through perfomance prediction experiments. These experiments show that the most discriminant speaker characteristics are -- first -- their total speech duration in the current show and -- then only -- the amount of training data available to build their acoustic model.
\end{abstract}

\noindent{\bf Index Terms}: speaker recognition, error analysis, TV broadcast

\section{Introduction}
\label{sec:introduction}
\input{introduction}

\begin{table*}[t]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    System          & PERCOL                                        & QCOMPERE                                        & SODA      \\
    \hline    
    Diarization     & two stage spk. diarization \cite{charlet2013} &  multi-stage spk. diarization \cite{Barras2006} & i-vector \cite{dupuy2014}         \\
                    & + overlapping speech detection                &                                                 &           \\
    \hline    
    SID             & ALIZE v3.0 toolkit \cite{larcher2013}         & Bob toolkit \cite{bob2012}                      & ALIZE v3.0 toolkit \cite{larcher2013}  \\
    \hline    
    feature         & 19 LFCC + $\delta$ coef,                      & 15 PLP-like cepstrum coef~\cite{Hermansky1990}  & 19 MFCC + $\delta$ coef,         \\
                    & + $\delta$ energy + 11 $\delta$$\delta$ coef  & 15 $\delta$ coef + $\delta$ energy              & + $\delta$ energy + 11 $\delta$$\delta$ coef  \\
    \hline    
    UBM             & gender independent                            & multi-lingual                                   & gender independent         \\
                    & 512 diagonal Gaussians                        & 256 diagonal Gaussians                          & 1024 diagonal Gaussians         \\
    \hline    
    i-vector        & 200 dim TVS estimated from                    & 400 dim TVS estimated from $39356$              & 300 dim TVS estimated from         \\
                    & 1200 spk. and 7500 sessions                   & speech segments (around $15$ seg./spk.)         & 680 spk. and 4150 sessions         \\
    \hline    
    Normalisation   & cepstral mean subtraction                     & Feature warping normalization                   & cepstral mean subtraction         \\
                    & and variance normalization                    & with a sliding window of $3$~s.~\cite{Pelecanos2001}.  &   and variance normalization        \\
    \hline    
    Training data   & 533 spk. id., min 30s, max 2mn30              & $706$ spk. id., min 30s                         & 680 spk. id., min 1mn , max 12 min         \\
    for i-vector    & (if higher, a set of i-vectors extracted)     & REPERE+ETAPE+French radio                       & REPERE+ETAPE+French radio+web         \\
                    & REPERE+ETAPE+French radio+web                 &                                                 &           \\            
    \hline    
    Decision        & CDS joined with Within-Class                  & PLDA                                            & PLDA         \\
                    & covariance normalization for                  & Eigen Factor Radial-based                       & Eigen Factor Radial-based       \\
                    & session/channel compensation                  & length normalization~\cite{bousquet2011}        &    length normalization      \\
    \hline                              
  \end{tabular}
  \caption{System comparison, TVS : total variability space, CDS: Cosine Distance Scoring}
  \label{tab:system}  
\end{table*}


\vspace*{-0.4cm}
\section{Experimental protocol}
\label{sec:protocol}
\input{protocol}

\section{Performance analysis}
\label{sec:analysis}
\input{analysis}

\section{Predicting speaker recognizability}
\label{sec:prediction}
\input{prediction}


\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

%\section{Acknowledgements}

\newpage
\eightpt

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
